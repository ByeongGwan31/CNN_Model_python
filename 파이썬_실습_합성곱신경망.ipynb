{"cells":[{"cell_type":"markdown","metadata":{"id":"3tH_w9ku7jyK"},"source":["# 예측 애널리틱스: 합성곱신경망 (출처 : https://github.com/DMQA/Python-codes-for-machine-learning-algorithms)"]},{"cell_type":"code","source":["## GIt 연결하기"],"metadata":{"id":"3f3ezOJn7tJJ","executionInfo":{"status":"ok","timestamp":1705422950601,"user_tz":-540,"elapsed":570,"user":{"displayName":"강병관","userId":"15477357221876048109"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-ArFwUA_ITLt","executionInfo":{"status":"ok","timestamp":1705422985702,"user_tz":-540,"elapsed":21579,"user":{"displayName":"강병관","userId":"15477357221876048109"}},"outputId":"32c05452-d239-4649-e8cf-2bbe87863e8f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["cd /content/drive/MyDrive/Colab_/CNN_Model_python"],"metadata":{"id":"ypNpGsbQK7WY","executionInfo":{"status":"ok","timestamp":1705423652098,"user_tz":-540,"elapsed":479,"user":{"displayName":"강병관","userId":"15477357221876048109"}},"outputId":"7259a0fc-b0cd-41ff-f1e8-982f26de8d63","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab_/CNN_Model_python\n"]}]},{"cell_type":"code","source":["!git config --global user.email 2000bk@naver.com\n","!git config --global user.name ByeongGwan31"],"metadata":{"id":"_7SdXjtjK8vT","executionInfo":{"status":"ok","timestamp":1705423664341,"user_tz":-540,"elapsed":944,"user":{"displayName":"강병관","userId":"15477357221876048109"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YjhynHRjLBEI"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-05-16T06:44:42.524571Z","start_time":"2021-05-16T06:44:42.484933Z"},"id":"NVXMjxJa7jyN"},"outputs":[],"source":["dataset = FashionMNIST(download_root, transform=fasion_mnist_transform, train=True, download=True)\n","X, y = dataset.data.numpy().reshape(-1, 28*28) , dataset.targets.numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-05-16T06:43:17.889979Z","start_time":"2021-05-16T06:43:14.795687Z"},"id":"d0Pr_iVp7jyO","outputId":"884b08f8-edf9-4226-c561-d3cf8ede1f2c"},"outputs":[{"data":{"text/plain":["46036"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["(model.predict(train_x.reshape(-1,28*28)) == train_y.numpy()).sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-05-16T06:42:27.452442Z","start_time":"2021-05-16T06:42:24.307977Z"},"id":"Mzh5licA7jyP","outputId":"ce1c394a-35e3-4cd8-8ab2-daf6a260006d"},"outputs":[{"data":{"text/plain":["False"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["(model.predict(train_x.reshape(-1,28*28)) == train_y)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-05-16T06:37:38.095569Z","start_time":"2021-05-16T06:37:36.386573Z"},"id":"IAmJ_3AU7jyQ"},"outputs":[],"source":["dataset = FashionMNIST(download_root, transform=fasion_mnist_transform, train=True, download=True)\n","X, y = dataset.data.numpy() , dataset.targets.numpy()\n","\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X.reshape(60000,28*28))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-05-16T06:35:42.427714Z","start_time":"2021-05-16T06:33:21.537473Z"},"id":"GdB8Eb2M7jyQ","outputId":"616012f6-4267-4510-f7ea-c4028b400858"},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m<ipython-input-14-82cd5d7ba57f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    384\u001b[0m             \u001b[1;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[1;31m# since correctness does not rely on using threads.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[0;32m    387\u001b[0m                              \u001b[1;33m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'threads'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m                 delayed(_parallel_build_trees)(\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1049\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    864\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 866\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    867\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    782\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 784\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    785\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    166\u001b[0m                                                         indices=indices)\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    888\u001b[0m         \"\"\"\n\u001b[0;32m    889\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 890\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m    891\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    373\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from sklearn.ensemble import RandomForestClassifier\n","model = RandomForestClassifier()\n","model.fit(X.reshape(-1,28*28),y)"]},{"cell_type":"markdown","metadata":{"id":"7gD5Boyb7jyQ"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"FuNxwvb77jyQ"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"DLQNg-mZ7jyQ"},"source":["## 1. 모듈 불러오기"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-05-16T06:30:38.300651Z","start_time":"2021-05-16T06:30:26.577597Z"},"id":"hUU38fw97jyR"},"outputs":[],"source":["# 실습용 데이터 패키지\n","from torchvision.datasets import FashionMNIST\n","\n","# 데이터 전처리 패키지\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.preprocessing import StandardScaler\n","\n","# 기계학습 모델 패키지 : 파이토치 라이토닝\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","#!pip install pytorch-lightning # Pytorch-lightning 패키지 다운로드 코드입니다 ! 없으시면 다운로드 하시죠 ~!\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","from pytorch_lightning.metrics import functional as FM\n","\n","\n","# 데이터 시각화 패키지\n","# %matplotlib inline\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import matplotlib.font_manager as fm\n","plt.rc('font', family='Malgun Gothic') # 한글 폰트 설정\n","\n","# 예측 평가 지표 패키지\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","\n","# 예측 결과 시각화 custom 함수\n","def plot_confusion_matrix(cm, classes,title, cmap=plt.cm.Blues):\n","\n","    plt.figure(figsize=(15,15))\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title,fontsize=30)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, cm[i, j],\n","                 horizontalalignment=\"center\",\n","                 color=\"red\" if cm[i, j] > thresh else \"black\",fontsize=30)\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    plt.show()\n","\n","# CNN filter 시각화 함수\n","def show_filter_filter(parameter_path):\n","\n","    # parameter load\n","    parameter = torch.load(parameter_path)['state_dict']\n","\n","    # conv1.weight load\n","    conv1 = parameter['conv1.weight'].detach().cpu().numpy().reshape(1,10,3,3)\n","    for chanel_ind , chanel in enumerate(conv1):\n","\n","        fig = plt.figure(figsize=(20,5))\n","        plt.title(f'CONV1 filter - type : {chanel_ind}')\n","        for ind, img in enumerate(chanel):\n","            ax = fig.add_subplot(2,5,ind + 1)\n","            ax.imshow(img, interpolation='nearest',cmap = 'gray')\n","            ax.set_xticks([]), ax.set_yticks([])\n","\n","            for i, j in itertools.product(range(img.shape[0]), range(img.shape[1])):\n","                plt.text(j, i, round(img[i, j],2),\n","                horizontalalignment=\"center\",color=\"red\",fontsize=12)\n","\n","    # conv2.weight load\n","    conv2 = parameter['conv2.weight'].detach().cpu().numpy().reshape(20,10,3,3)\n","    for chanel_ind , chanel in enumerate(conv2):\n","\n","        fig = plt.figure(figsize=(20,5))\n","        plt.title(f'CONV2 filter - type : {chanel_ind}')\n","        for ind, img in enumerate(chanel):\n","            ax = fig.add_subplot(2,5,ind + 1)\n","            ax.imshow(img, interpolation='nearest',cmap = 'gray')\n","            ax.set_xticks([]), ax.set_yticks([])\n","\n","            for i, j in itertools.product(range(img.shape[0]), range(img.shape[1])):\n","                plt.text(j, i, round(img[i, j],2),\n","                horizontalalignment=\"center\",color=\"red\",fontsize=12)\n","\n","\n","    # conv3.weight load\n","    conv3 = parameter['conv3.weight'].detach().cpu().numpy().reshape(40,20,3,3)\n","    for chanel_ind , chanel in enumerate(conv3):\n","\n","        fig = plt.figure(figsize=(20,10))\n","        plt.title(f'CONV3 filter - type : {chanel_ind}')\n","        for ind, img in enumerate(chanel):\n","            ax = fig.add_subplot(4,5,ind + 1)\n","            ax.imshow(img, interpolation='nearest',cmap = 'gray')\n","            ax.set_xticks([]), ax.set_yticks([])\n","\n","            for i, j in itertools.product(range(img.shape[0]), range(img.shape[1])):\n","                plt.text(j, i, round(img[i, j],2),\n","                horizontalalignment=\"center\",color=\"red\",fontsize=12)\n","\n","    plt.show()\n","\n","# 기타\n","import warnings, itertools, time\n","warnings.filterwarnings(action='ignore')"]},{"cell_type":"markdown","metadata":{"id":"9ngLdfoi7jyS"},"source":["## 2. 데이터 불러오고 로더 만들기"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-05-16T06:30:38.486023Z","start_time":"2021-05-16T06:30:38.345603Z"},"id":"g3jc9tym7jyS"},"outputs":[],"source":["# Fashion MNIST 데이터를 저장할 directory\n","download_root = './'\n","\n","# 데이터 scaling을 위한 요소\n","fasion_mnist_transform = transforms.Compose([\n","    transforms.ToTensor(), # 데이터 형태를 Pytorch에 적합한 형태로 바꾸어 줌\n","    transforms.Normalize((0.0,), (1.0,)) # feature들의 평균과 표준편차를 0과 1로 scaling 해줌\n","])\n","\n","#FashionMNIST 훈련 데이터셋 다운로드\n","dataset = FashionMNIST(download_root, transform=fasion_mnist_transform, train=True, download=True)\n","\n","# 훈련 데이터셋을 50000개(학습용) / 10000개(검증용)으로 분리\n","train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [50000, 10000])\n","\n","# 테스트 데이터셋 다운로드\n","test_dataset = FashionMNIST(download_root, transform=fasion_mnist_transform, train=False, download=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-04-09T07:16:46.037683Z","start_time":"2021-04-09T07:16:46.026191Z"},"id":"02RtiW307jyT"},"outputs":[],"source":["# 데이터를 100개 단위의 미니 배치로 구분지어 인공신경망 학습\n","train_loader = DataLoader(train_dataset,batch_size=100)\n","valid_loader = DataLoader(valid_dataset,batch_size=100)\n","test_loader = DataLoader(test_dataset,batch_size=100)"]},{"cell_type":"markdown","metadata":{"id":"azmusnNy7jyT"},"source":["## 3. Pytorch-lightning을 이용한 CNN 모델 구현"]},{"cell_type":"markdown","metadata":{"id":"o7-WUq8C7jyT"},"source":["### 3.1 모델 구축"]},{"cell_type":"markdown","metadata":{"id":"DvsAVSDm7jyT"},"source":["#### CNN"]},{"cell_type":"markdown","metadata":{"id":"iduB31Vp7jyT"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-04-09T05:16:44.041665Z","start_time":"2021-04-09T05:16:43.999780Z"},"id":"Tyil2BOQ7jyT"},"outputs":[],"source":["class CNN(pl.LightningModule):\n","    def __init__(self, class_num):\n","        super().__init__()\n","\n","        # convolution\n","        self.conv1 = nn.Conv2d(1,10,(3,3))\n","        self.conv2 = nn.Conv2d(10,20,(3,3))\n","        self.conv3 = nn.Conv2d(20,40,(3,3))\n","\n","        # pooling layer\n","        self.max_pool = nn.MaxPool2d((2,2),2)\n","\n","        # dropout\n","        self.dropout_1 = nn.Dropout(p=0.3)\n","        self.dropout_2 = nn.Dropout(p=0.1)\n","\n","        # activation function\n","        self.relu = nn.ReLU(inplace=True)\n","        self.lrelu = nn.LeakyReLU(negative_slope=0.1,inplace=True)\n","\n","        # clasifier\n","        self.classifier = nn.Sequential(nn.Linear(500, 300),\n","                                        self.relu,\n","                                        self.dropout_1,\n","                                        nn.Linear(300,100),\n","                                        self.lrelu,\n","                                        self.dropout_2,\n","                                        nn.Linear(100, class_num))\n","\n","        # for logging\n","        self.train_loss_ = []\n","        self.train_acc_ = []\n","        self.val_loss_ = []\n","        self.val_acc_ = []\n","\n","        # for convenient\n","        self.class_num = class_num\n","\n","    def forward(self,x): # x is input data\n","\n","        # x -> convolution -> relu -> max_pool -> dropout\n","        x = self.conv1(x)\n","        x = self.relu(x)\n","        x = self.max_pool(x)\n","        x = self.dropout_1(x)\n","\n","        # .. -> convolution -> relu -> max_pool -> dropout\n","        x = self.conv2(x)\n","        x = self.lrelu(x)\n","        x = self.max_pool(x)\n","        x = self.dropout_2(x)\n","\n","        # .. -> classifier -> logit\n","        flatten = x.view(x.size(0),-1)\n","        logit = self.classifier(flatten)\n","\n","        return logit\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters() , lr = 0.001)\n","        return optimizer\n","\n","    def training_step(self,batch,batch_idx):\n","        x, y = batch\n","        logit = self(x) # self(x)는 model(input)와 같다고 이해하시면 됩니다.\n","        prob = F.softmax(logit,dim=1)\n","        train_accuracy = FM.accuracy(prob,y)\n","        loss = F.cross_entropy(logit,y)\n","        logs = {'train_loss':loss,'train_acc':train_accuracy}\n","        result = {'loss':loss,'log':logs,'train_acc':train_accuracy}\n","        return result\n","\n","    def training_epoch_end(self,result):\n","        avg_loss = torch.stack([x['loss'] for x in result]).mean()\n","        avg_acc = torch.stack([x['train_acc'] for x in result]).mean()\n","        epoch_result = {'loss':avg_loss,'train_acc':avg_acc}\n","        self.train_loss_.append(avg_loss.cpu().numpy().item())\n","        self.train_acc_.append(avg_acc.cpu().numpy().item())\n","        return epoch_result\n","\n","    def validation_step(self,batch,batch_idx):\n","        x, y = batch\n","        y_hat = self(x)\n","        y_hat_prob = F.softmax(y_hat,dim=1)\n","        val_accuracy = FM.accuracy(y_hat_prob,y)\n","        val_loss = F.cross_entropy(y_hat,y)\n","        logs = {'val_accuracy':val_accuracy,'val_loss':val_loss}\n","        result = {'loss':val_loss,'log':logs,'val_acc':val_accuracy}\n","        return result\n","\n","    def validation_epoch_end(self,result):\n","        avg_loss = torch.stack([x['loss'] for x in result]).mean()\n","        avg_acc = torch.stack([x['val_acc'] for x in result]).mean()\n","        epoch_result = {'loss':avg_loss,'acc':avg_acc}\n","        self.val_loss_.append(avg_loss.cpu().numpy().item())\n","        self.val_acc_.append(avg_acc.cpu().numpy().item())\n","        self.log('val_accuracy',avg_acc)\n","        return epoch_result\n","\n","    def test_step(self,batch,batch_idx):\n","        x, y = batch\n","        y_hat = self(x)\n","        y_hat_prob = F.softmax(y_hat,dim=1)\n","        result = {'predicted':y_hat_prob,'target':y}\n","        return result\n","\n","    def test_epoch_end(self,result):\n","\n","        predicted = torch.stack([x['predicted'] for x in result])\n","        predicted = predicted.view(-1,self.class_num)\n","        target = torch.stack([x['target'] for x in result])\n","        target = target.view(-1)\n","\n","        self.test_predicted = predicted.detach().cpu().numpy()\n","        self.test_target= target.detach().cpu().numpy()"]},{"cell_type":"markdown","metadata":{"id":"b_erjPOv7jyU"},"source":["#### DNN"]},{"cell_type":"markdown","metadata":{"id":"2emjvfWh7jyU"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-04-09T05:59:38.565011Z","start_time":"2021-04-09T05:59:38.531589Z"},"id":"PCOawiMH7jyU"},"outputs":[],"source":["class DNN(pl.LightningModule):\n","    def __init__(self, class_num):\n","        super().__init__()\n","\n","        # dropout\n","        self.dropout_1 = nn.Dropout(p=0.3)\n","        self.dropout_2 = nn.Dropout(p=0.1)\n","\n","        # activation function\n","        self.relu = nn.ReLU(inplace=True)\n","        self.lrelu = nn.LeakyReLU(negative_slope=0.1,inplace=True)\n","\n","        # clasifier\n","        self.classifier = nn.Sequential(nn.Linear(28*28, 300),\n","                                        self.relu,\n","                                        self.dropout_1,\n","                                        nn.Linear(300,100),\n","                                        self.lrelu,\n","                                        self.dropout_2,\n","                                        nn.Linear(100, class_num))\n","\n","        # for logging\n","        self.train_loss_ = []\n","        self.train_acc_ = []\n","        self.val_loss_ = []\n","        self.val_acc_ = []\n","\n","        # for convenient\n","        self.class_num = class_num\n","\n","    def forward(self,x): # x is input data\n","\n","        # x -> classifier -> logit\n","        flatten = x.view(x.size(0),-1)\n","        logit = self.classifier(flatten)\n","\n","        return logit\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters() , lr = 0.001)\n","        return optimizer\n","\n","    def training_step(self,batch,batch_idx):\n","        x, y = batch\n","        logit = self(x)\n","        prob = F.softmax(logit,dim=1)\n","        train_accuracy = FM.accuracy(prob,y)\n","        loss = F.cross_entropy(logit,y)\n","        logs = {'train_loss':loss,'train_acc':train_accuracy}\n","        result = {'loss':loss,'log':logs,'train_acc':train_accuracy}\n","        return result\n","\n","    def training_epoch_end(self,result):\n","        avg_loss = torch.stack([x['loss'] for x in result]).mean()\n","        avg_acc = torch.stack([x['train_acc'] for x in result]).mean()\n","        epoch_result = {'loss':avg_loss,'train_acc':avg_acc}\n","        self.train_loss_.append(avg_loss.cpu().numpy().item())\n","        self.train_acc_.append(avg_acc.cpu().numpy().item())\n","        return epoch_result\n","\n","    def validation_step(self,batch,batch_idx):\n","        x, y = batch\n","        y_hat = self(x)\n","        y_hat_prob = F.softmax(y_hat,dim=1)\n","        val_accuracy = FM.accuracy(y_hat_prob,y)\n","        val_loss = F.cross_entropy(y_hat,y)\n","        logs = {'val_accuracy':val_accuracy,'val_loss':val_loss}\n","        result = {'loss':val_loss,'log':logs,'val_acc':val_accuracy}\n","        return result\n","\n","    def validation_epoch_end(self,result):\n","        avg_loss = torch.stack([x['loss'] for x in result]).mean()\n","        avg_acc = torch.stack([x['val_acc'] for x in result]).mean()\n","        epoch_result = {'loss':avg_loss,'acc':avg_acc}\n","        self.val_loss_.append(avg_loss.cpu().numpy().item())\n","        self.val_acc_.append(avg_acc.cpu().numpy().item())\n","        self.log('val_accuracy',avg_acc)\n","        return epoch_result\n","\n","    def test_step(self,batch,batch_idx):\n","        x, y = batch\n","        y_hat = self(x)\n","        y_hat_prob = F.softmax(y_hat,dim=1)\n","        result = {'predicted':y_hat_prob,'target':y}\n","        return result\n","\n","    def test_epoch_end(self,result):\n","\n","        predicted = torch.stack([x['predicted'] for x in result])\n","        predicted = predicted.view(-1,self.class_num)\n","        target = torch.stack([x['target'] for x in result])\n","        target = target.view(-1)\n","\n","        self.test_predicted = predicted.detach().cpu().numpy()\n","        self.test_target= target.detach().cpu().numpy()"]},{"cell_type":"markdown","metadata":{"id":"s5WZJh5f7jyV"},"source":["### 3.2 모델 학습"]},{"cell_type":"markdown","metadata":{"id":"qujFyRLF7jyV"},"source":["#### CNN"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-04-09T05:47:20.522066Z","start_time":"2021-04-09T05:16:44.106514Z"},"id":"XPv-XkOA7jyV"},"outputs":[],"source":["start = time.time()  # 모델 학습 시작 시간 저장\n","cnn = CNN(class_num=10)\n","early_stop = EarlyStopping(monitor='val_accuracy',patience=5,verbose=True,mode='max') # Early stopping\n","checkpoint = ModelCheckpoint(filename='./CNN-{epoch}-{val_accuracy}',monitor='val_accuracy',mode='max',save_top_k=1) # Model parameter save\n","trainer = pl.Trainer(callbacks=[early_stop,checkpoint],gpus=0) # 컴퓨터에 GPU가 있으신 분들은 0을 1로 변경하세요 ! 지피유 이용이 가능합니다.\n","trainer.fit(cnn,train_loader,valid_loader)\n","print(\"time :\", time.time() - start) # 학습 소요 시간 출력"]},{"cell_type":"markdown","metadata":{"id":"N4PmRiwn7jyV"},"source":["#### DNN"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-04-09T06:22:48.854441Z","start_time":"2021-04-09T05:59:48.658019Z"},"id":"_alrHRk47jyV"},"outputs":[],"source":["start = time.time()  # 모델 학습 시작 시간 저장\n","dnn = DNN(class_num=10)\n","early_stop = EarlyStopping(monitor='val_accuracy',patience=5,verbose=True,mode='max') # Early stopping\n","checkpoint = ModelCheckpoint(filename='./DNN-{epoch}-{val_accuracy}',monitor='val_accuracy',mode='max',save_top_k=1) # Model parameter save\n","trainer = pl.Trainer(callbacks=[early_stop,checkpoint],gpus=0)\n","trainer.fit(dnn,train_loader,valid_loader)\n","print(\"time :\", time.time() - start) # 학습 소요 시간 출력"]},{"cell_type":"markdown","metadata":{"id":"m4Q_o8L-7jyV"},"source":["### 3.3 학습 현황 확인 및 예측 결과 출력"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-04-09T07:42:50.121439Z","start_time":"2021-04-09T07:42:48.347345Z"},"id":"uJUsYX137jyW"},"outputs":[],"source":["# CNN logs\n","cnn_train_loss = cnn.train_loss_\n","cnn_train_acc = cnn.train_acc_\n","cnn_valid_loss = cnn.val_loss_\n","cnn_valid_acc = cnn.val_acc_\n","\n","# DNN logs\n","dnn_train_loss = dnn.train_loss_\n","dnn_train_acc = dnn.train_acc_\n","dnn_valid_loss = dnn.val_loss_\n","dnn_valid_acc = dnn.val_acc_\n","\n","# 그래프 배경화면 만들기\n","f, axs = plt.subplots(4,1,figsize=(20,30))\n","\n","# 학습로스 변동 관찰하기\n","axs[0].plot(cnn_train_loss,label='CNN 학습 로스')\n","axs[0].plot(dnn_train_loss,label='DNN 학습 로스')\n","axs[0].legend()\n","\n","# 학습 정확도 변동 관찰하기\n","axs[1].plot(cnn_train_acc,label='CNN 학습 정확도')\n","axs[1].plot(dnn_train_acc,label='DNN 학습 정확도')\n","axs[1].legend()\n","\n","# 검증용 데이터에 대한 Accuracy 변동 관찰하기\n","axs[2].plot(cnn_valid_acc,label='CNN검증 정확도')\n","axs[2].scatter(np.argmax(cnn_valid_acc),np.max(cnn_valid_acc),s=100,facecolors='none',edgecolors='r',\n","              label='CNN 최적의 학습 체크포인트(=학습 중단점)')\n","axs[2].plot(dnn_valid_acc,label='DNN검증 정확도')\n","axs[2].scatter(np.argmax(dnn_valid_acc),np.max(dnn_valid_acc),s=100,facecolors='none',edgecolors='r',\n","              label='DNN 최적의 학습 체크포인트(=학습 중단점)')\n","axs[2].legend()\n","\n","\n","# 검증용 데이터에 대한 loss 변동 관찰하기\n","axs[3].plot(cnn_valid_loss,label='CNN 검증 로스')\n","axs[3].plot(dnn_valid_loss,label='DNN 검증 로스')\n","axs[3].legend()\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-04-09T07:45:10.031828Z","start_time":"2021-04-09T07:44:54.978274Z"},"id":"I6Pn3nPb7jyW"},"outputs":[],"source":["# test Accuracy 확인하기\n","\n","trainer.test(model=cnn, test_dataloaders=test_loader) # test data에 대한 예측 수행\n","trainer.test(model=dnn, test_dataloaders=test_loader) # test data에 대한 예측 수행\n","\n","cnn_test_accuracy = accuracy_score(y_pred=cnn.test_predicted.argmax(1),y_true=cnn.test_target)\n","dnn_test_accuracy = accuracy_score(y_pred=dnn.test_predicted.argmax(1),y_true=dnn.test_target)\n","print(f\"CNN 분류 정확도: {cnn_test_accuracy:.3f}\",f\"DNN 분류 정확도: {dnn_test_accuracy:.3f}\",sep='  |  ')"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-04-09T07:45:35.454192Z","start_time":"2021-04-09T07:45:32.467916Z"},"id":"xXqr0GR87jyW"},"outputs":[],"source":["confusion_matrix_ = confusion_matrix(y_pred=cnn.test_predicted.argmax(1),y_true=cnn.test_target)\n","plot_confusion_matrix(cm = confusion_matrix_, classes=dataset.classes,title='CNN을 이용한 Fashion MNIST 예측 결과')"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-04-09T06:24:42.106719Z","start_time":"2021-04-09T06:24:40.320157Z"},"id":"HkJNoGwp7jyW"},"outputs":[],"source":["confusion_matrix_ = confusion_matrix(y_pred=dnn.test_predicted.argmax(1),y_true=dnn.test_target)\n","plot_confusion_matrix(cm = confusion_matrix_, classes=dataset.classes,title='DNN을 이용한 Fashion MNIST 예측 결과')"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-04-09T06:25:58.474087Z","start_time":"2021-04-09T06:25:52.532721Z"},"id":"gM-1Omlz7jyW"},"outputs":[],"source":["for_visualization_test_x = test_dataset.data.numpy()[:100]\n","for_visualization_test_y = test_dataset.targets.numpy()[:100]\n","\n","f ,axs = plt.subplots(10,10,figsize=(20,20))\n","plt.subplots_adjust(hspace=0.8)\n","for i in range(10):\n","    for j in range(10):\n","\n","        x_data = for_visualization_test_x[10*i+j]\n","        y_data = for_visualization_test_y[10*i+j]\n","        real_class = dataset.classes[y_data]\n","        predicted_class = dataset.classes[cnn.test_predicted[10*i+j].argmax()]\n","        predicted_prob = cnn.test_predicted[10*i+j].max()\n","\n","        # x, y 축의 지점 표시를 안함\n","        axs[i,j].set_xticks([])\n","        axs[i,j].set_yticks([])\n","\n","        # subplot의 제목을 i번째 결과에 해당하는 숫자로 설정\n","        axs[i,j].set_title(f\"실제 class:{real_class}\\n예측 class:{predicted_class}\\nProbability:{predicted_prob:.2f}\")\n","\n","        # 입력으로 사용한 i번째 테스트 이미지를 28x28로 재배열하고\n","        # 이 2차원 배열을 그레이스케일 이미지로 출력\n","        axs[i,j].imshow(x_data.reshape((28, 28)),cmap=plt.cm.gray_r)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-04-09T06:30:02.773500Z","start_time":"2021-04-09T06:27:45.335481Z"},"id":"tg0-Xo0i7jyW"},"outputs":[],"source":["parameter_path = \"./lightning_logs/version_11/checkpoints/CNN-epoch=34-val_accuracy=0.9134000539779663.ckpt\"\n","show_filter_filter(parameter_path)"]},{"cell_type":"markdown","metadata":{"id":"XFlKQ3577jyX"},"source":["# EOD"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"257.271px"},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}